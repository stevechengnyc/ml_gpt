{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82b898b7",
   "metadata": {},
   "source": [
    "# üîç LLM Text Summarization: Top 10 Colab Questions\n",
    "This notebook covers the most frequently asked coding questions about using Large Language Models (LLMs) for text summarization.\n",
    "\n",
    "Each section includes code, best practices, and comments for easy understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daffd42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Install required packages (for Google Colab)\n",
    "!pip install transformers datasets rouge-score fastapi uvicorn[standard] bitsandbytes accelerate --quiet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793a88cd",
   "metadata": {},
   "source": [
    "## 1. Summarize Text with Hugging Face BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc1f338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Load a BART model pre-trained for summarization\n",
    "from transformers import pipeline\n",
    "\n",
    "# Create a summarization pipeline\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Input text\n",
    "text = \"Long article text goes here...\"\n",
    "\n",
    "# Generate the summary\n",
    "summary = summarizer(text, max_length=130, min_length=30, do_sample=False)\n",
    "print(\"üìù Summary:\", summary[0]['summary_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59be233a",
   "metadata": {},
   "source": [
    "## 2. Fine-tune BART on CNN/DailyMail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff4e465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Fine-tune BART using Hugging Face `Trainer` API\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, TrainingArguments, Trainer\n",
    "\n",
    "# Load a small portion of the dataset for demonstration\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"train[:1%]\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "# Preprocess function for summarization\n",
    "def preprocess(examples):\n",
    "    inputs = tokenizer(examples[\"article\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "    targets = tokenizer(examples[\"highlights\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_dataset = dataset.map(preprocess, batched=True)\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "# Training configuration\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=10\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=tokenized_dataset)\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d90ef1",
   "metadata": {},
   "source": [
    "## 3. Extractive vs. Abstractive Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0ee9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Extractive summarization with spaCy (highlights original sentences)\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from heapq import nlargest\n",
    "\n",
    "text = \"Long article text here...\"\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "\n",
    "# Calculate word frequencies\n",
    "word_freq = {}\n",
    "for word in doc:\n",
    "    if word.text.lower() not in STOP_WORDS and word.is_alpha:\n",
    "        word_freq[word.text.lower()] = word_freq.get(word.text.lower(), 0) + 1\n",
    "\n",
    "# Score sentences based on word frequency\n",
    "sentence_scores = {}\n",
    "for sent in doc.sents:\n",
    "    for word in sent:\n",
    "        if word.text.lower() in word_freq:\n",
    "            sentence_scores[sent] = sentence_scores.get(sent, 0) + word_freq[word.text.lower()]\n",
    "\n",
    "# Extract top 3 sentences\n",
    "summary_sentences = nlargest(3, sentence_scores, key=sentence_scores.get)\n",
    "summary = \" \".join([sent.text for sent in summary_sentences])\n",
    "print(\"üìù Extractive Summary:\", summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ca8616",
   "metadata": {},
   "source": [
    "## 4. Summarize Long Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d3bca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Handle long documents using chunking\n",
    "def split_text(text, chunk_size=400):\n",
    "    words = text.split()\n",
    "    for i in range(0, len(words), chunk_size):\n",
    "        yield \" \".join(words[i:i + chunk_size])\n",
    "\n",
    "# Example input\n",
    "long_text = \"Very long document text...\"\n",
    "\n",
    "chunks = list(split_text(long_text))\n",
    "\n",
    "from transformers import pipeline\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Summarize each chunk\n",
    "summary_parts = [summarizer(chunk, max_length=130, min_length=30, do_sample=False)[0]['summary_text'] for chunk in chunks]\n",
    "full_summary = \" \".join(summary_parts)\n",
    "print(\"üìù Full Summary:\", full_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a840826c",
   "metadata": {},
   "source": [
    "## 5. ROUGE Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef5f253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Evaluate summarization quality using ROUGE metric\n",
    "from datasets import load_metric\n",
    "\n",
    "rouge = load_metric(\"rouge\")\n",
    "\n",
    "# Example prediction and reference\n",
    "predictions = [\"The company posted strong revenue growth and plans expansion.\"]\n",
    "references = [\"The company reported revenue increase and future expansion.\"]\n",
    "\n",
    "# Compute ROUGE scores\n",
    "results = rouge.compute(predictions=predictions, references=references)\n",
    "print(\"üìä ROUGE Scores:\", results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d959bc5",
   "metadata": {},
   "source": [
    "## 6. Prompt-based Summarization (Chat Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256ee359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Summarize using chat/instruction-tuned LLMs\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "\n",
    "prompt = \"Summarize this article:\\n\" + \"Long article...\" + \"\\nSummary:\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "output_ids = model.generate(input_ids, max_new_tokens=150)\n",
    "summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(\"üìù Prompt-based Summary:\", summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cffee78",
   "metadata": {},
   "source": [
    "## 7. Batch Summarization from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30108dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Load and summarize articles from CSV\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "df = pd.read_csv(\"articles.csv\")  # Assume column: 'content'\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Generate summaries for each row\n",
    "df[\"summary\"] = df[\"content\"].apply(lambda x: summarizer(x, max_length=130, min_length=30, do_sample=False)[0]['summary_text'])\n",
    "df.to_csv(\"summaries.csv\", index=False)\n",
    "print(\"‚úÖ Summaries saved to summaries.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a86ff26",
   "metadata": {},
   "source": [
    "## 8. REST API with FastAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1b071b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Build a summarization REST API with FastAPI\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from transformers import pipeline\n",
    "\n",
    "app = FastAPI()\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "class TextRequest(BaseModel):\n",
    "    text: str\n",
    "\n",
    "@app.post(\"/summarize\")\n",
    "def summarize(req: TextRequest):\n",
    "    result = summarizer(req.text, max_length=130, min_length=30, do_sample=False)\n",
    "    return {\"summary\": result[0]['summary_text']}\n",
    "\n",
    "# ‚û§ To run: save as app.py and run `uvicorn app:app --reload`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adc3807",
   "metadata": {},
   "source": [
    "## 9. Quantized Summarization (4-bit LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0c6ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Use quantized LLMs for memory-efficient summarization\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"TheBloke/LLaMA-2-7B-GGML\", quantization_config=bnb_config, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TheBloke/LLaMA-2-7B-GGML\")\n",
    "\n",
    "# Inference would proceed as usual using tokenizer and model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99de584f",
   "metadata": {},
   "source": [
    "## 10. Multilingual Summarization (mBART)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052cd5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Summarize multilingual text using mBART\n",
    "from transformers import MBartTokenizer, MBartForConditionalGeneration\n",
    "\n",
    "model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-cc25\")\n",
    "tokenizer = MBartTokenizer.from_pretrained(\"facebook/mbart-large-cc25\")\n",
    "\n",
    "text = \"Texte en fran√ßais ici...\"  # French input\n",
    "tokenizer.src_lang = \"fr_XX\"\n",
    "\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512).input_ids\n",
    "summary_ids = model.generate(input_ids, max_length=100)\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "print(\"üìù French Summary:\", summary)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
