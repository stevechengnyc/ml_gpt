{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be48a55f",
   "metadata": {},
   "source": [
    "## 1. Summarize Text with Hugging Face BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6554c03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "text = \"Long article text goes here...\"\n",
    "summary = summarizer(text, max_length=130, min_length=30, do_sample=False)\n",
    "print(summary[0]['summary_text'])\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be428e3",
   "metadata": {},
   "source": [
    "## 2. Fine-tune BART on CNN/DailyMail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46765d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, TrainingArguments, Trainer\n",
    "\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"train[:1%]\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "def preprocess(examples):\n",
    "    inputs = tokenizer(examples[\"article\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "    targets = tokenizer(examples[\"highlights\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess, batched=True)\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\", per_device_train_batch_size=4, num_train_epochs=1, logging_steps=10\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc55477b",
   "metadata": {},
   "source": [
    "## 3. Extractive vs. Abstractive Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdc270a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extractive with spaCy\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from heapq import nlargest\n",
    "\n",
    "text = \"Long article text here...\"\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "\n",
    "word_freq = {}\n",
    "for word in doc:\n",
    "    if word.text.lower() not in STOP_WORDS and word.is_alpha:\n",
    "        word_freq[word.text.lower()] = word_freq.get(word.text.lower(), 0) + 1\n",
    "\n",
    "sentence_scores = {}\n",
    "for sent in doc.sents:\n",
    "    for word in sent:\n",
    "        if word.text.lower() in word_freq:\n",
    "            sentence_scores[sent] = sentence_scores.get(sent, 0) + word_freq[word.text.lower()]\n",
    "\n",
    "summary_sentences = nlargest(3, sentence_scores, key=sentence_scores.get)\n",
    "summary = \" \".join([sent.text for sent in summary_sentences])\n",
    "print(summary)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cbfdf4",
   "metadata": {},
   "source": [
    "## 4. Summarize Long Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50eb835",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_text(text, chunk_size=1024):\n",
    "    words = text.split()\n",
    "    for i in range(0, len(words), chunk_size):\n",
    "        yield \" \".join(words[i:i + chunk_size])\n",
    "\n",
    "chunks = list(split_text(\"Long document text\", chunk_size=400))\n",
    "\n",
    "from transformers import pipeline\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "summary_parts = [summarizer(chunk, max_length=130, min_length=30, do_sample=False)[0]['summary_text'] for chunk in chunks]\n",
    "full_summary = \" \".join(summary_parts)\n",
    "print(full_summary)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606446df",
   "metadata": {},
   "source": [
    "## 5. ROUGE Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a2478d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_metric\n",
    "\n",
    "rouge = load_metric(\"rouge\")\n",
    "predictions = [\"Bart summarized this well.\"]\n",
    "references = [\"The summary should cover key points clearly.\"]\n",
    "results = rouge.compute(predictions=predictions, references=references)\n",
    "print(results)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7436b1c2",
   "metadata": {},
   "source": [
    "## 6. Prompt-based Summarization (Chat Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fba83b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "\n",
    "prompt = \"Summarize this article:\\n\" + \"Long article...\" + \"\\nSummary:\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "output_ids = model.generate(input_ids, max_new_tokens=150)\n",
    "summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(summary)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a49b0e",
   "metadata": {},
   "source": [
    "## 7. Batch Summarization from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d71de2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "df = pd.read_csv(\"articles.csv\")  # column: content\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "df[\"summary\"] = df[\"content\"].apply(lambda x: summarizer(x, max_length=130, min_length=30, do_sample=False)[0]['summary_text'])\n",
    "df.to_csv(\"summaries.csv\", index=False)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae6143f",
   "metadata": {},
   "source": [
    "## 8. REST API with FastAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5740a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save this as app.py\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from transformers import pipeline\n",
    "\n",
    "app = FastAPI()\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "class TextRequest(BaseModel):\n",
    "    text: str\n",
    "\n",
    "@app.post(\"/summarize\")\n",
    "def summarize(req: TextRequest):\n",
    "    result = summarizer(req.text, max_length=130, min_length=30, do_sample=False)\n",
    "    return {\"summary\": result[0]['summary_text']}\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6228acaf",
   "metadata": {},
   "source": [
    "## 9. Quantized Summarization (4-bit LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ec342d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"TheBloke/LLaMA-2-7B-GGML\", quantization_config=bnb_config, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TheBloke/LLaMA-2-7B-GGML\")\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bdd6a6",
   "metadata": {},
   "source": [
    "## 10. Multilingual Summarization (mBART)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de784c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import MBartTokenizer, MBartForConditionalGeneration\n",
    "\n",
    "model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-cc25\")\n",
    "tokenizer = MBartTokenizer.from_pretrained(\"facebook/mbart-large-cc25\")\n",
    "\n",
    "text = \"Texte en fran√ßais ici...\"  # French text\n",
    "tokenizer.src_lang = \"fr_XX\"\n",
    "\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512).input_ids\n",
    "summary_ids = model.generate(input_ids, max_length=100)\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "print(summary)\n",
    "     "
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
