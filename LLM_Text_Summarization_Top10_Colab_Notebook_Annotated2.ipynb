{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "82b898b7",
      "metadata": {
        "id": "82b898b7"
      },
      "source": [
        "# üîç LLM Text Summarization: Top 10 Colab Questions\n",
        "This notebook covers the most frequently asked coding questions about using Large Language Models (LLMs) for text summarization.\n",
        "\n",
        "Each section includes code, best practices, and comments for easy understanding."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q google-cloud-aiplatform"
      ],
      "metadata": {
        "id": "22gAl771UG3r"
      },
      "id": "22gAl771UG3r",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install additional packages if they're not installed.\n",
        "!pip install google-generativeai\n",
        "!pip install protobuf\n",
        "\n",
        "from google.colab import auth\n",
        "from google.auth import default\n",
        "\n",
        "auth.authenticate_user()\n",
        "creds, _ = default()\n",
        "\n",
        "# Provide system-level instruction\n",
        "import google.generativeai as genai\n",
        "genai.configure(api_key=creds.token)"
      ],
      "metadata": {
        "id": "fr3_X6wcZp00",
        "outputId": "5637767b-bc96-44aa-bca0-87ed8a1d59b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 897
        }
      },
      "id": "fr3_X6wcZp00",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.4)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.24.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.164.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.4)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.11.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.13.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.69.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.4.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.1.31)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (5.29.4)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-2bea5846007c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauth\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mauth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthenticate_user\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mcreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/auth.py\u001b[0m in \u001b[0;36mauthenticate_user\u001b[0;34m(clear_output, project_id)\u001b[0m\n\u001b[1;32m    258\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_check_adc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_CredentialType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUSER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muse_auth_ephem\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m       _message.blocking_request(\n\u001b[0m\u001b[1;32m    261\u001b[0m           \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m           \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'auth_user_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u50VRjlhZp4_"
      },
      "id": "u50VRjlhZp4_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import chat\n",
        "\n",
        "# Provide system-level instruction\n",
        "chat.set_system_instruction(\"You are a clinical assistant generating post-op care notes.\")\n",
        "\n",
        "# Define your few-shot prompt\n",
        "prompt = \"\"\"\n",
        "Example 1:\n",
        "Patient: John Doe, Procedure: Laparoscopic Appendectomy, Date: 04/10/2025. Vitals stable, ambulating, tolerating diet.\n",
        "Post-Op Note:\n",
        "Patient John Doe underwent laparoscopic appendectomy on 04/10/2025. He is currently stable, ambulating independently, and tolerating oral intake. No signs of infection or complication. Continue standard post-op care and reassess in 24 hours.\n",
        "\n",
        "Example 2:\n",
        "Patient: Jane Smith, Procedure: Total Knee Replacement, Date: 04/09/2025. Mild pain, using walker, PT initiated.\n",
        "Post-Op Note:\n",
        "Patient Jane Smith is post-op day 2 following total knee replacement. She reports mild pain managed with oral analgesics. Mobilizing with walker, and physical therapy has been initiated. No concerning findings.\n",
        "\n",
        "Case:\n",
        "Patient: Alex Kim, Procedure: Hernia Repair, Date: 04/11/2025. Awake, mild nausea, dressing clean.\n",
        "Post-Op Note:\n",
        "\"\"\"\n",
        "\n",
        "# Send the prompt to Gemini\n",
        "response = chat.chat(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "Y-MqRfCrUG7h",
        "outputId": "4ce00841-f92a-422b-a97c-f5912198040a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        }
      },
      "id": "Y-MqRfCrUG7h",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'chat' from 'google.colab' (/usr/local/lib/python3.11/dist-packages/google/colab/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-ce7e4c5d406b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mchat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Provide system-level instruction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mchat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_system_instruction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You are a clinical assistant generating post-op care notes.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'chat' from 'google.colab' (/usr/local/lib/python3.11/dist-packages/google/colab/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZFzNy0-RUG_L"
      },
      "id": "ZFzNy0-RUG_L",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "74o055CZUHCl"
      },
      "id": "74o055CZUHCl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jfpYxx1jUHFj"
      },
      "id": "jfpYxx1jUHFj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eS1lirKNUHIu"
      },
      "id": "eS1lirKNUHIu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "daffd42e",
      "metadata": {
        "id": "daffd42e"
      },
      "outputs": [],
      "source": [
        "# üì¶ Install required packages (for Google Colab)\n",
        "!pip install transformers datasets rouge-score fastapi uvicorn[standard] bitsandbytes accelerate --quiet\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "793a88cd",
      "metadata": {
        "id": "793a88cd"
      },
      "source": [
        "## 1. Summarize Text with Hugging Face BART"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdc1f338",
      "metadata": {
        "id": "fdc1f338"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ Load a BART model pre-trained for summarization\n",
        "from transformers import pipeline\n",
        "\n",
        "# Create a summarization pipeline\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "# Input text\n",
        "text = \"Long article text goes here...\"\n",
        "\n",
        "# Generate the summary\n",
        "summary = summarizer(text, max_length=130, min_length=30, do_sample=False)\n",
        "print(\"üìù Summary:\", summary[0]['summary_text'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59be233a",
      "metadata": {
        "id": "59be233a"
      },
      "source": [
        "## 2. Fine-tune BART on CNN/DailyMail"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ff4e465",
      "metadata": {
        "id": "7ff4e465"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ Fine-tune BART using Hugging Face `Trainer` API\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, TrainingArguments, Trainer\n",
        "\n",
        "# Load a small portion of the dataset for demonstration\n",
        "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"train[:1%]\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n",
        "\n",
        "# Preprocess function for summarization\n",
        "def preprocess(examples):\n",
        "    inputs = tokenizer(examples[\"article\"], truncation=True, padding=\"max_length\", max_length=512)\n",
        "    targets = tokenizer(examples[\"highlights\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
        "    return inputs\n",
        "\n",
        "# Tokenize the dataset\n",
        "tokenized_dataset = dataset.map(preprocess, batched=True)\n",
        "\n",
        "# Load the model\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-base\")\n",
        "\n",
        "# Training configuration\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_train_batch_size=4,\n",
        "    num_train_epochs=1,\n",
        "    logging_steps=10\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer = Trainer(model=model, args=training_args, train_dataset=tokenized_dataset)\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3d90ef1",
      "metadata": {
        "id": "d3d90ef1"
      },
      "source": [
        "## 3. Extractive vs. Abstractive Summarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b0ee9ed",
      "metadata": {
        "id": "6b0ee9ed"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ Extractive summarization with spaCy (highlights original sentences)\n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from heapq import nlargest\n",
        "\n",
        "text = \"Long article text here...\"\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(text)\n",
        "\n",
        "# Calculate word frequencies\n",
        "word_freq = {}\n",
        "for word in doc:\n",
        "    if word.text.lower() not in STOP_WORDS and word.is_alpha:\n",
        "        word_freq[word.text.lower()] = word_freq.get(word.text.lower(), 0) + 1\n",
        "\n",
        "# Score sentences based on word frequency\n",
        "sentence_scores = {}\n",
        "for sent in doc.sents:\n",
        "    for word in sent:\n",
        "        if word.text.lower() in word_freq:\n",
        "            sentence_scores[sent] = sentence_scores.get(sent, 0) + word_freq[word.text.lower()]\n",
        "\n",
        "# Extract top 3 sentences\n",
        "summary_sentences = nlargest(3, sentence_scores, key=sentence_scores.get)\n",
        "summary = \" \".join([sent.text for sent in summary_sentences])\n",
        "print(\"üìù Extractive Summary:\", summary)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73ca8616",
      "metadata": {
        "id": "73ca8616"
      },
      "source": [
        "## 4. Summarize Long Documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8d3bca5",
      "metadata": {
        "id": "a8d3bca5"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ Handle long documents using chunking\n",
        "def split_text(text, chunk_size=400):\n",
        "    words = text.split()\n",
        "    for i in range(0, len(words), chunk_size):\n",
        "        yield \" \".join(words[i:i + chunk_size])\n",
        "\n",
        "# Example input\n",
        "long_text = \"Very long document text...\"\n",
        "\n",
        "chunks = list(split_text(long_text))\n",
        "\n",
        "from transformers import pipeline\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "# Summarize each chunk\n",
        "summary_parts = [summarizer(chunk, max_length=130, min_length=30, do_sample=False)[0]['summary_text'] for chunk in chunks]\n",
        "full_summary = \" \".join(summary_parts)\n",
        "print(\"üìù Full Summary:\", full_summary)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a840826c",
      "metadata": {
        "id": "a840826c"
      },
      "source": [
        "## 5. ROUGE Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aef5f253",
      "metadata": {
        "id": "aef5f253"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ Evaluate summarization quality using ROUGE metric\n",
        "from datasets import load_metric\n",
        "\n",
        "rouge = load_metric(\"rouge\")\n",
        "\n",
        "# Example prediction and reference\n",
        "predictions = [\"The company posted strong revenue growth and plans expansion.\"]\n",
        "references = [\"The company reported revenue increase and future expansion.\"]\n",
        "\n",
        "# Compute ROUGE scores\n",
        "results = rouge.compute(predictions=predictions, references=references)\n",
        "print(\"üìä ROUGE Scores:\", results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d959bc5",
      "metadata": {
        "id": "7d959bc5"
      },
      "source": [
        "## 6. Prompt-based Summarization (Chat Models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "256ee359",
      "metadata": {
        "id": "256ee359"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ Summarize using chat/instruction-tuned LLMs\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
        "\n",
        "prompt = \"Summarize this article:\\n\" + \"Long article...\" + \"\\nSummary:\"\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "output_ids = model.generate(input_ids, max_new_tokens=150)\n",
        "summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "print(\"üìù Prompt-based Summary:\", summary)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cffee78",
      "metadata": {
        "id": "4cffee78"
      },
      "source": [
        "## 7. Batch Summarization from CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30108dda",
      "metadata": {
        "id": "30108dda"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ Load and summarize articles from CSV\n",
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "\n",
        "df = pd.read_csv(\"articles.csv\")  # Assume column: 'content'\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "# Generate summaries for each row\n",
        "df[\"summary\"] = df[\"content\"].apply(lambda x: summarizer(x, max_length=130, min_length=30, do_sample=False)[0]['summary_text'])\n",
        "df.to_csv(\"summaries.csv\", index=False)\n",
        "print(\"‚úÖ Summaries saved to summaries.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a86ff26",
      "metadata": {
        "id": "3a86ff26"
      },
      "source": [
        "## 8. REST API with FastAPI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d1b071b",
      "metadata": {
        "id": "4d1b071b"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ Build a summarization REST API with FastAPI\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "from transformers import pipeline\n",
        "\n",
        "app = FastAPI()\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "class TextRequest(BaseModel):\n",
        "    text: str\n",
        "\n",
        "@app.post(\"/summarize\")\n",
        "def summarize(req: TextRequest):\n",
        "    result = summarizer(req.text, max_length=130, min_length=30, do_sample=False)\n",
        "    return {\"summary\": result[0]['summary_text']}\n",
        "\n",
        "# ‚û§ To run: save as app.py and run `uvicorn app:app --reload`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2adc3807",
      "metadata": {
        "id": "2adc3807"
      },
      "source": [
        "## 9. Quantized Summarization (4-bit LLM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c0c6ef7",
      "metadata": {
        "id": "4c0c6ef7"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ Use quantized LLMs for memory-efficient summarization\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\"TheBloke/LLaMA-2-7B-GGML\", quantization_config=bnb_config, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"TheBloke/LLaMA-2-7B-GGML\")\n",
        "\n",
        "# Inference would proceed as usual using tokenizer and model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99de584f",
      "metadata": {
        "id": "99de584f"
      },
      "source": [
        "## 10. Multilingual Summarization (mBART)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "052cd5de",
      "metadata": {
        "id": "052cd5de"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ Summarize multilingual text using mBART\n",
        "from transformers import MBartTokenizer, MBartForConditionalGeneration\n",
        "\n",
        "model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-cc25\")\n",
        "tokenizer = MBartTokenizer.from_pretrained(\"facebook/mbart-large-cc25\")\n",
        "\n",
        "text = \"Texte en fran√ßais ici...\"  # French input\n",
        "tokenizer.src_lang = \"fr_XX\"\n",
        "\n",
        "input_ids = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512).input_ids\n",
        "summary_ids = model.generate(input_ids, max_length=100)\n",
        "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "print(\"üìù French Summary:\", summary)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}